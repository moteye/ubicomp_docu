% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{bretzner2005towards,
  Title                    = {Towards low-cost systems for measuring visual cues of driver fatigue and inattention in automotive applications},
  Author                   = {Bretzner, Lars and Krantz, Martin},
  Booktitle                = {Vehicular Electronics and Safety, 2005. IEEE International Conference on},
  Year                     = {2005},
  Organization             = {IEEE},
  Pages                    = {161--164},

  Abstract                 = {Recent studies show that driver fatigue and
inattention are major causes of fatal road accidents. A large
number of these accidents could be avoided if the vehicles
were equipped with a) sensors that reliably could monitor the
attention and alertness cues of the drivers, and b) systems that,
based on the sensor input, could warn the drivers in time.
Today, camera-based systems can measure such attention cues,
and are tested in automotive in-cabin installations with two or
more cameras. Camera-based solutions are non-intrusive, they
can measure head position and orientation as well as gaze
direction and eyelid closure, and they can be used to identify
the driver for other tasks. However until now, no camera-based
system has met the requirements from the car manufacturers on
compactness and cost. Here we present the principles behind
Smart Eye AntiSleep, which is a compact one-camera system
specially designed for automotive in-cabin applications.
AntiSleep measures 3D head position, head orientation, gaze
direction and eyelid closures. AntiSleep uses one camera and
two IR-flashes. With its own illumination, AntiSleep is highly
robust to all natural illumination conditions and can efficiently
handle disturbing reflexes in eyeglasses. The system is fully
automatic and requires no manual intervention during initial-
isation.},
  File                     = {:papers/10.1.1.137.3735.pdf:PDF},
  Owner                    = {tamino},
  Timestamp                = {2014.05.11}
}

@InProceedings{jacob1990you,
  Title                    = {What you look at is what you get: eye movement-based interaction techniques},
  Author                   = {Jacob, Robert JK},
  Booktitle                = {Proceedings of the SIGCHI conference on Human factors in computing systems},
  Year                     = {1990},
  Organization             = {ACM},
  Pages                    = {11--18},

  Abstract                 = {In seeking hitherto-unused methods by which users and computers can communicate, we investigate the usefulness of eye movements as a fast and convenient auxiliary user-to-computer communication mode. The barrier to exploiting this medium has not been eye-tracking technology but the study of interaction techniques that incorporate eye movements into the user-computer dialogue in a natural and unobtrusive way. This paper discusses some of the human factors and technical considerations that arise in trying to use eye movements as an input medium, describes our approach and the first eye movement-based interaction techniques that we have devised and implemented in our laboratory, and reports our experiences and observations on them.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.14}
}

@Article{ji2002real,
  Title                    = {Real-time eye, gaze, and face pose tracking for monitoring driver vigilance},
  Author                   = {Ji, Qiang and Yang, Xiaojie},
  Journal                  = {Real-Time Imaging},
  Year                     = {2002},
  Number                   = {5},
  Pages                    = {357--377},
  Volume                   = {8},

  Abstract                 = {This paper describes a real-time prototype computer vision system for monitoring driver
vigilance. The main components of the system consists of a remotely located video CCD
camera, a specially designed hardware system for real-time image acquisition and for
controlling the illuminator and the alarm system, and various computer vision algorithms for
simultaneously, real-time and non-intrusively monitoring various visual bio-behaviors that
typically characterize a driver’s level of vigilance. The visual behaviors include eyelid movement,
face orientation, and gaze movement (pupil movement). The system was tested in a simulating
environment with subjects of different ethnic backgrounds, different genders, ages, with/without
glasses, and under different illumination conditions, and it was found very robust, reliable and
accurate.},
  File                     = {:papers/10.1.1.408.8589.pdf:PDF},
  Owner                    = {tamino},
  Publisher                = {Elsevier},
  Timestamp                = {2014.05.11}
}

@InProceedings{liu2002real,
  Title                    = {Real-time eye detection and tracking for driver observation under various light conditions},
  Author                   = {Liu, Xia and Xu, Fengliang and Fujimura, Kikuo},
  Booktitle                = {Intelligent Vehicle Symposium, 2002. IEEE},
  Year                     = {2002},
  Organization             = {IEEE},
  Pages                    = {344--351},
  Volume                   = {2},

  Abstract                 = {Eye tracking is one of the key technologies for future driver assistance systems since human eyes contain much information about the driver's condition such as gaze, attention level, and fatigue level. Thus, nonintrusive methods for eye detection and tracking are important for many applications of vision-based driver-automotive interaction. One problem common to many eye tracking methods proposed so far is their sensitivity to lighting condition change. This tends to significantly limit their scope for automotive applications. In this paper we present a new realtime eye detection and tracking method that works under variable and realistic lighting conditions. By combining imaging by using IR light and appearance-based object recognition techniques, our method can robustly track eyes even when the pupils are not very bright due to significant external illumination interferences. The appearance model is incorporated in both eye detection and tracking via the use of a support vector machine and mean shift tracking. Our experimental results show the feasibility of our approach and the validity for the method is extended for drivers wearing sunglasses.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.11}
}

@InProceedings{velichkovsky1997towards,
  Title                    = {Towards gaze-mediated interaction: Collecting solutions of the “Midas touch problem”},
  Author                   = {Velichkovsky, Boris and Sprenger, Andreas and Unema, Pieter},
  Booktitle                = {Human-Computer Interaction INTERACT’97},
  Year                     = {1997},
  Organization             = {Springer},
  Pages                    = {509--516},

  Abstract                 = {For a development of truly user-centered interfaces we need to take into account not only generic characteristics of human beings but also actual dynamics of attention and intentions of persons involved in an interaction. Modern eyetracking methods are indispensable tools in such a development, as they allow the use of eye movement data for control of output devices, for gaze-contingent image processing and for desambiguation of verbal as well as nonverbal information. The main obstacle on the way to these applications is the so-called “Midas touch problem”: how to differentiate “attentive” saccades with intended goal of communication from the lower level eye movements that are just random or provoked by external stimulation? We report results of our investigations of the problem and present a solution based on a functional classification of fixations correlated with their duration. Several additional solutions are also considered together with the data on the trainability of the human oculomotor system.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.14}
}

