% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Alt:2014:UES:2557500.2557518,
  Title                    = {Using Eye-tracking to Support Interaction with Layered 3D Interfaces on Stereoscopic Displays},
  Author                   = {Alt, Florian and Schneegass, Stefan and Auda, Jonas and Rzayev, Rufat and Broy, Nora},
  Booktitle                = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
  Year                     = {2014},

  Address                  = {New York, NY, USA},
  Pages                    = {267--272},
  Publisher                = {ACM},
  Series                   = {IUI '14},

  Abstract                 = {In this paper, we investigate the concept of gaze-based interaction with 3D user interfaces. We currently see stereo vision displays becoming ubiquitous, particularly as auto-stereoscopy enables the perception of 3D content without the use of glasses. As a result, application areas for 3D beyond entertainment in cinema or at home emerge, including work settings, mobile phones, public displays, and cars. At the same time, eye tracking is hitting the consumer market with low-cost devices. We envision eye trackers in the future to be integrated with consumer devices (laptops, mobile phones, displays), hence allowing the user's gaze to be analyzed and used as input for interactive applications. A particular challenge when applying this concept to 3D displays is that current eye trackers provide the gaze point in 2D only (x and y coordinates). In this paper, we compare the performance of two methods that use the eye's physiology for calculating the gaze point in 3D space, hence enabling gaze-based interaction with stereoscopic content. Furthermore, we provide a comparison of gaze interaction in 2D and 3D with regard to user experience and performance. Our results show that with current technology, eye tracking on stereoscopic displays is possible with similar performance as on standard 2D screens.},
  Acmid                    = {2557518},
  Doi                      = {10.1145/2557500.2557518},
  ISBN                     = {978-1-4503-2184-6},
  Keywords                 = {3d, eye tracking, gaze interaction, stereoscopic displays},
  Location                 = {Haifa, Israel},
  Numpages                 = {6},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.25},
  Url                      = {http://doi.acm.org/10.1145/2557500.2557518}
}

@InProceedings{althoff2005vdi,
  Title                    = {Robust Multimodal Hand- and Head Gesture Recognition for
controlling Automotive Infotainment Systems},
  Author                   = {F. Althoff and R. Lindl and L. Walchshaeusl},
  Booktitle                = {VDI-Tagung: Der Fahrer im 21. Jahrhundert},
  Year                     = {2005},

  Address                  = {Braunschweig, Germany},
  Month                    = {November 22-23},

  Abstract                 = {The use of gestures in automotive environments provides an intuitive addition to existing interaction styles for seamlessly controlling various infotainment applications like radio- tuner, cd-player and telephone. In this work, we describe a robust, context-specific approach for a video-based analysis of dynamic hand- and head gestures. The system, implemented in a BMW limousine, evaluates a continuous stream of infrared pictures using a combination of adapted preprocessing methods and a hierarchical, mainly rule based classification scheme. Currently, 17 different hand gestures and six different head gestures can be recognized in realtime on standard hardware. As a key-feature of the system, the active gesture vocabulary can be reduced with regard to the current operating context yielding more robust performance.},
  Key                      = {Althoff et al.},
  Owner                    = {Cassio},
  Review                   = {Information exchange between human and machine seems highly artificial
multimodal interfaces have the potential to be more robust
free user choice
gesture based can be used in noisy environment vs. speech

video based analysis of hand and head gestures, integrated in a multimodal architecture
Gestures can increase both comfort and driving safety since the eyes can focus on the road
Head
head gestures: nodding and shaking -> decision
hand gestures: skip between cd tracks or radio stations, shortcut functions
17 hand, 6 head},
  Timestamp                = {2014.05.15}
}

@InProceedings{bretzner2005towards,
  Title                    = {Towards low-cost systems for measuring visual cues of driver fatigue and inattention in automotive applications},
  Author                   = {Bretzner, Lars and Krantz, Martin},
  Booktitle                = {Vehicular Electronics and Safety, 2005. IEEE International Conference on},
  Year                     = {2005},
  Organization             = {IEEE},
  Pages                    = {161--164},

  Abstract                 = {Recent studies show that driver fatigue and inattention are major causes of fatal road accidents. A large number of these accidents could be avoided if the vehicles were equipped with a) sensors that reliably could monitor the attention and alertness cues of the drivers, and b) systems that, based on the sensor input, could warn the drivers in time. Today, camera-based systems can measure such attention cues, and are tested in automotive in-cabin installations with two or more cameras. Camera-based solutions are non-intrusive, they can measure head position and orientation as well as gaze direction and eyelid closure, and they can be used to identify the driver for other tasks. However until now, no camera-based system has met the requirements from the car manufacturers on compactness and cost. Here we present the principles behind Smart Eye AntiSleep, which is a compact one-camera system specially designed for automotive in-cabin applications. AntiSleep measures 3D head position, head orientation, gaze direction and eyelid closures. AntiSleep uses one camera and two IR-flashes. With its own illumination, AntiSleep is highly robust to all natural illumination conditions and can efficiently handle disturbing reflexes in eyeglasses. The system is fully automatic and requires no manual intervention during initialisation.},
  File                     = {:papers/10.1.1.137.3735.pdf:PDF},
  Owner                    = {tamino},
  Timestamp                = {2014.05.11}
}

@InProceedings{Broy:2006:IMI:1133265.1133297,
  Title                    = {iFlip: A Metaphor for In-vehicle Information Systems},
  Author                   = {Broy, Verena and Althoff, Frank and Klinker, Gudrun},
  Booktitle                = {Proceedings of the Working Conference on Advanced Visual Interfaces},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {155--158},
  Publisher                = {ACM},
  Series                   = {AVI '06},

  Abstract                 = {After the successful transfer of hierarchical menu-structures from the computer domain to an automotive environment, it is time to discuss the potential of 3D metaphors to meet the strong requirements for in-vehicle information systems (IVIS). The idea is to increase learnability, efficiency and joy of use of IVIS by providing a 3D interaction concept that is based on cognitive capabilities of humans. We present a 3D interaction metaphor, iFlip, which consists of displaying information on the reverse side of thin interaction objects and a preview to current submenu states.A comparison with a traditional list-based 2D menu for IVIS has shown that iFlip fulfills automotive requirements and can even enhance usability and likeability of IVIS.},
  Acmid                    = {1133297},
  Doi                      = {10.1145/1133265.1133297},
  ISBN                     = {1-59593-353-0},
  Keywords                 = {3D interaction, 3D metaphor, automotive infotainment, spatial memory, vision-based UI},
  Location                 = {Venezia, Italy},
  Numpages                 = {4},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.25},
  Url                      = {http://doi.acm.org/10.1145/1133265.1133297}
}

@InProceedings{Ecker:2010:VCS:1969773.1969788,
  Title                    = {Visual Cues Supporting Direct Touch Gesture Interaction with In-vehicle Information Systems},
  Author                   = {Ecker, Ronald and Broy, Verena and Hertzschuch, Katja and Butz, Andreas},
  Booktitle                = {Proceedings of the 2Nd International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {80--87},
  Publisher                = {ACM},
  Series                   = {AutomotiveUI '10},

  Abstract                 = {Recent in-vehicle information systems are increasingly equipped with touch screens. While classic (i.e. point-based) direct touch interaction has known benefits in non-automotive environments, it primarily relies on visual attention, which makes it a bad candidate for interaction in the car, where visual attention should be on the road. We have designed an interaction scheme for IVIS based on touch gestures and pie menus and implemented several versions of it featuring visual cues as improvements to the original idea. In an extensive user study with a primary driving task, we were able to show that our interaction scheme is significantly faster than classic touch interaction and that it demands shorter gesture using visual cues.},
  Acmid                    = {1969788},
  Doi                      = {10.1145/1969773.1969788},
  ISBN                     = {978-1-4503-0437-5},
  Keywords                 = {automotive HCI, automotive user studies, in-vehicle information system (IVIS), pie menu, touch gestures, touch screens},
  Location                 = {Pittsburgh, Pennsylvania},
  Numpages                 = {8},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.25},
  Url                      = {http://doi.acm.org/10.1145/1969773.1969788}
}

@Article{Fletcher:2009:DID:1542017.1542026,
  Title                    = {Driver Inattention Detection Based on Eye Gaze-Road Event Correlation},
  Author                   = {Fletcher, Luke and Zelinsky, Alexander},
  Journal                  = {Int. J. Rob. Res.},
  Year                     = {2009},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {774--801},
  Volume                   = {28},

  Abstract                 = {Current road safety initiatives are approaching the limit of their effectiveness in developed countries. A paradigm shift is needed to address the preventable deaths of thousands on our roads. Previous systems have focused on one or two aspects of driving: environmental sensing, vehicle dynamics or driver monitoring. Our approach is to consider the driver and the vehicle as part of a combined system, operating within the road environment. A driver assistance system is implemented that is not only responsive to the road environment and the driver's actions but also designed to correlate the driver's eye gaze with road events to determine the driver's observations. Driver observation monitoring enables an immediate in-vehicle system able to detect and act on driver inattentiveness, providing the precious seconds for an inattentive human driver to react. We present a prototype system capable of estimating the driver's observations and detecting driver inattentiveness. Due to the "look but not see" case it is not possible to prove that a road event has been observed by the driver. We show, however, that it is possible to detect missed road events and warn the driver appropriately.},
  Acmid                    = {1542026},
  Address                  = {Thousand Oaks, CA, USA},
  Doi                      = {10.1177/0278364908099459},
  ISSN                     = {0278-3649},
  Issue_date               = {June 2009},
  Numpages                 = {28},
  Owner                    = {Cassio},
  Publisher                = {Sage Publications, Inc.},
  Timestamp                = {2014.05.25},
  Url                      = {http://dx.doi.org/10.1177/0278364908099459}
}

@Article{horrey2004driving,
  Title                    = {Driving and side task performance: The effects of display clutter, separation, and modality},
  Author                   = {Horrey, William J and Wickens, Christopher D},
  Journal                  = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {611--624},
  Volume                   = {46},

  Abstract                 = {In-vehicle technologies (IVTs) create additional tasks for the driver. To the extent that these devices degrade driving performance, there will be safety concerns. This study examines the effects of display clutter from overlay, display separation, and modality on driving and IVT task performance. In a fixed-base simulator, 22 drivers drove different routes and responded to infrequent, unexpected road hazards while engaging in a phone number task presented by different displays. Visual displays were located on a head-up (overlaid on the visual horizon or adjacently, just above the vehicle hood) or head-down display (HDD) located near the midconsole. Alternatively, digits were presented auditorily. In general, there were no differences in performance for the adjacent and overlay displays; however, there were costs associated with the HDD and auditory display for some measures. In particular, responses to hazard events were slowed when drivers used the HDD. Overall, the adjacent display best supported performance on all relevant tasks. Potential applications of this research include the design of IVTs with respect to location and modality.},
  Doi                      = {10.1518/hfes.46.4.611.56805},
  Owner                    = {Cassio},
  Publisher                = {Sage Publications},
  Review                   = {General Motors},
  Timestamp                = {2014.05.15}
}

@InProceedings{huckauf2005you,
  Title                    = {What you don't look at is what you get: anti-saccades can reduce the midas touch-problem},
  Author                   = {Huckauf, Anke and Goettel, Timo and Heinbockel, Malte and Urbina, Mario},
  Booktitle                = {Proceedings of the 2nd symposium on Applied perception in graphics and visualization},
  Year                     = {2005},
  Organization             = {ACM},
  Pages                    = {170--170},

  Abstract                 = {Controlling computers by eyes can provide a fast and efficient alternative to the computer mouse. However, in gaze controlled systems object selection is still suboptimal: Dwell times on a certain object typically used to elicit the selection of this object can cause unintended selections; a problem known as Midas Touch. For our suggested alternative approach using anti-saccades for selection, highlighted objects are copied to one side of the object. The object is selected when fixating to the side opposed to that copy requiring to inhibit an automatic response towards new objects. A user study revealed shorter task completion times, but more errors when selecting objects by anti-saccades relative to selection by individually and task-specifically adapted dwell times. With certain improvements, anti-saccades can become a serious alternative to dwell times.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.15}
}

@Misc{iti1999method,
  Title                    = {Method and apparatus for automatically configuring a system based on a user's monitored system interaction and preferred system access times},

  Author                   = {Iti, Jean M Goldschmidt and Hackson, David N and Moore, Kenneth Alan and Shah-Nazaroff, Anthony A and Watts, E Michael and Williams, Christopher D},
  Month                    = nov # {~2},
  Note                     = {US Patent 5,977,964},
  Year                     = {1999},

  Abstract                 = {In accordance with the teachings of the present invention, a method and apparatus for automatically configuring a system based on a user's monitored system interaction and preferred system access times is provided. According to one embodiment, a user profile corresponding to the user is updated based at least in part on the monitored user interaction with the system. Preferred system access times of the user are identified based at least in part on the user profile, and the system is automatically configured based at least in part on the user profile and the user's preferred system access times.},
  Owner                    = {tamino},
  Publisher                = {Google Patents},
  Timestamp                = {2014.05.15}
}

@InProceedings{jacob1990you,
  Title                    = {What you look at is what you get: eye movement-based interaction techniques},
  Author                   = {Jacob, Robert JK},
  Booktitle                = {Proceedings of the SIGCHI conference on Human factors in computing systems},
  Year                     = {1990},
  Organization             = {ACM},
  Pages                    = {11--18},

  Abstract                 = {In seeking hitherto-unused methods by which users and computers can communicate, we investigate the usefulness of eye movements as a fast and convenient auxiliary user-to-computer communication mode. The barrier to exploiting this medium has not been eye-tracking technology but the study of interaction techniques that incorporate eye movements into the user-computer dialogue in a natural and unobtrusive way. This paper discusses some of the human factors and technical considerations that arise in trying to use eye movements as an input medium, describes our approach and the first eye movement-based interaction techniques that we have devised and implemented in our laboratory, and reports our experiences and observations on them.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.14}
}

@Article{ji2002real,
  Title                    = {Real-time eye, gaze, and face pose tracking for monitoring driver vigilance},
  Author                   = {Ji, Qiang and Yang, Xiaojie},
  Journal                  = {Real-Time Imaging},
  Year                     = {2002},
  Number                   = {5},
  Pages                    = {357--377},
  Volume                   = {8},

  Abstract                 = {This paper describes a real-time prototype computer vision system for monitoring driver vigilance. The main components of the system consists of a remotely located video CCD camera, a specially designed hardware system for real-time image acquisition and for controlling the illuminator and the alarm system, and various computer vision algorithms for simultaneously, real-time and non-intrusively monitoring various visual bio-behaviors that typically characterize a driver’s level of vigilance. The visual behaviors include eyelid movement, face orientation, and gaze movement (pupil movement). The system was tested in a simulating environment with subjects of different ethnic backgrounds, different genders, ages, with/without glasses, and under different illumination conditions, and it was found very robust, reliable and accurate.},
  File                     = {:papers/10.1.1.408.8589.pdf:PDF},
  Owner                    = {tamino},
  Publisher                = {Elsevier},
  Timestamp                = {2014.05.11}
}

@InProceedings{Kern:2010:MUD:1969773.1969792,
  Title                    = {Making Use of Drivers' Glances Onto the Screen for Explicit Gaze-based Interaction},
  Author                   = {Kern, Dagmar and Mahr, Angela and Castronovo, Sandro and Schmidt, Albrecht and M\"{u}ller, Christian},
  Booktitle                = {Proceedings of the 2Nd International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {110--116},
  Publisher                = {ACM},
  Series                   = {AutomotiveUI '10},

  Abstract                 = {Interaction with communication and infotainment systems in the car is common while driving. Our research investigates modalities and techniques that enable interaction with interactive applications while driving without compromising safety. In this paper we present the results of an experiment where we use eye-gaze tracking in combination with a button on the steering wheel as explicit input substituting the interaction on the touch screen. This approach combines the advantages of direct interaction on visual displays without the drawbacks of touch screens. In particular the freedom of placement for the screen (even out of reach from the user) and that both hands remain on the steering wheel are the main advantages. The results show that this interaction modality is slightly slower and more distracting than a touch screen but it is significantly faster than automated speech interaction.},
  Acmid                    = {1969792},
  Doi                      = {10.1145/1969773.1969792},
  ISBN                     = {978-1-4503-0437-5},
  Keywords                 = {automotive, eye tracking, modality choice, timing},
  Location                 = {Pittsburgh, Pennsylvania},
  Numpages                 = {7},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.25},
  Url                      = {http://doi.acm.org/10.1145/1969773.1969792}
}

@InProceedings{kern2009design,
  Title                    = {Design space for driver-based automotive user interfaces},
  Author                   = {Kern, Dagmar and Schmidt, Albrecht},
  Booktitle                = {Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
  Year                     = {2009},
  Organization             = {ACM},
  Pages                    = {3--10},

  Abstract                 = {Over the last 100 years it has become much easier to operate a car. However in recent years the number of functions a user can control while driving has greatly increased. Infotainment, entertainment and comfort systems as well as driver assistance contribute to this trend. Interaction with these systems plays an important role, as on one hand this can improve the user experience while driving but on the other hand it may distract from the primary task of driving. User interfaces in cars differ regarding the number of input and output devices and their placement in the car to a great extent. In this paper, we introduce a first design space for driver-based automotive user interfaces that allows a comprehensive description of input and output devices in a car with regard to placement and modality. This design space is intended to provide a basis for analyzing and discussing different user interface arrangements in cars, to compare alternative user interface setups, and to identify new opportunities for interaction and placement of controls. We present a graphical representation of the design space and discuss its usage in detail based on several examples. To assess the completeness of the proposed design space we used it to classify and compare user interfaces from more than 100 cars shown at IAA2007, cars from the BMW museum, and from the A2Mac1 image database.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.15}
}

@Article{Lethaus:2013:CSS:2527816.2528107,
  Title                    = {A Comparison of Selected Simple Supervised Learning Algorithms to Predict Driver Intent Based on Gaze Data},
  Author                   = {Lethaus, Firas and Baumann, Martin R. K. and K\"{o}ster, Frank and Lemmer, Karsten},
  Journal                  = {Neurocomput.},
  Year                     = {2013},

  Month                    = dec,
  Pages                    = {108--130},
  Volume                   = {121},

  Abstract                 = {Gaze behaviour is known to indicate information gathering. It is therefore suggested that it could be used to derive information about the driver's next planned objective in order to identify intended manoeuvres without relying solely on car data. Ultimately this would be practically realised by an Advanced Driver Assistance System (ADAS) using gaze data to correctly infer the intentions of the driver from what is implied by the incoming gaze data available to it. Neural Networks' ability to approximate arbitrary functions from observed data therefore makes them a candidate for modelling driver intent. Previous work has shown that significantly distinct gaze patterns precede each of the driving manoeuvres analysed indicating that eye movement data might be used as input to ADAS supplementing sensors, such as CAN-Bus (Controller Area Network), laser, radar or LIDAR (Light Detection and Ranging) in order to recognise intended driving manoeuvres. In this study, drivers' gaze behaviour was measured prior to and during the execution of different driving manoeuvres performed in a dynamic driving simulator. Artificial Neural Networks (ANNs), Bayesian Networks (BNs), and Naive Bayes Classifiers (NBCs) were then trained using gaze data to act as classifiers that predict the occurrence of certain driving manoeuvres. This has previously been successfully demonstrated with real traffic data [1]. Issues considered here included the amount of data that is used for predictive purposes prior to the manoeuvre, the accuracy of the predictive models at different times prior to the manoeuvre taking place and the relative difficulty of predicting a lane change left manoeuvre against predicting a lane change right manoeuvre.},
  Acmid                    = {2528107},
  Address                  = {Amsterdam, The Netherlands, The Netherlands},
  Doi                      = {10.1016/j.neucom.2013.04.035},
  ISSN                     = {0925-2312},
  Issue_date               = {December, 2013},
  Keywords                 = {Artificial Neural Networks, Bayesian Networks, Driver intent, Eye tracking, Naive Bayes Classifiers, Supervised learning},
  Numpages                 = {23},
  Owner                    = {Cassio},
  Publisher                = {Elsevier Science Publishers B. V.},
  Timestamp                = {2014.05.25},
  Url                      = {http://dx.doi.org/10.1016/j.neucom.2013.04.035}
}

@InProceedings{Lethaus:2011:UPR:1997052.1997069,
  Title                    = {Using Pattern Recognition to Predict Driver Intent},
  Author                   = {Lethaus, Firas and Baumann, Martin R. K. and K\"{o}ster, Frank and Lemmer, Karsten},
  Booktitle                = {Proceedings of the 10th International Conference on Adaptive and Natural Computing Algorithms - Volume Part I},
  Year                     = {2011},

  Address                  = {Berlin, Heidelberg},
  Pages                    = {140--149},
  Publisher                = {Springer-Verlag},
  Series                   = {ICANNGA'11},

  Abstract                 = {Advanced Driver Assistance Systems (ADAS) should correctly infer the intentions of the driver from what is implied by the incoming data available to it. Gaze behaviour has been found to be an indicator of information gathering, and therefore could be used to derive information about the driver's next planned objective in order to identify intended manoeuvres without relying solely on car data. Previous work has shown that significantly distinct gaze patterns precede each of the driving manoeuvres analysed indicating that eye movement data might be used as input to ADAS supplementing sensors, such as CAN-Bus, laser, or radar in order to recognise intended driving manoeuvres. Drivers' gaze behaviour was measured prior to and during the execution of different driving manoeuvres performed in a dynamic driving simulator. The efficacy of Artificial Neural Network models in learning to predict the occurrence of certain driving manoeuvres using both car and gaze data was investigated, which could successfully be demonstrated with real traffic data [1]. Issues considered included the amount of data prior to the manoeuvre to use, the relative difficulty of predicting different manoeuvres, and the accuracy of the models at different pre-manoeuvre times.},
  Acmid                    = {1997069},
  ISBN                     = {978-3-642-20281-0},
  Keywords                 = {ROC curves, artificial neural networks, driver intent, driving manoeuvres, eye tracking, machine learning, pattern recognition, signal detection theory},
  Location                 = {Ljubljana, Slovenia},
  Numpages                 = {10},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.25},
  Url                      = {http://dl.acm.org/citation.cfm?id=1997052.1997069}
}

@InProceedings{liu2002real,
  Title                    = {Real-time eye detection and tracking for driver observation under various light conditions},
  Author                   = {Liu, Xia and Xu, Fengliang and Fujimura, Kikuo},
  Booktitle                = {Intelligent Vehicle Symposium, 2002. IEEE},
  Year                     = {2002},
  Organization             = {IEEE},
  Pages                    = {344--351},
  Volume                   = {2},

  Abstract                 = {Eye tracking is one of the key technologies for future driver assistance systems since human eyes contain much information about the driver's condition such as gaze, attention level, and fatigue level. Thus, nonintrusive methods for eye detection and tracking are important for many applications of vision-based driver-automotive interaction. One problem common to many eye tracking methods proposed so far is their sensitivity to lighting condition change. This tends to significantly limit their scope for automotive applications. In this paper we present a new realtime eye detection and tracking method that works under variable and realistic lighting conditions. By combining imaging by using IR light and appearance-based object recognition techniques, our method can robustly track eyes even when the pupils are not very bright due to significant external illumination interferences. The appearance model is incorporated in both eye detection and tracking via the use of a support vector machine and mean shift tracking. Our experimental results show the feasibility of our approach and the validity for the method is extended for drivers wearing sunglasses.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.11}
}

@Article{liu2003effects,
  Title                    = {Effects of using head-up display in automobile context on attention demand and driving performance},
  Author                   = {Liu, Yung-Ching},
  Journal                  = {Displays},
  Year                     = {2003},
  Number                   = {4},
  Pages                    = {157--165},
  Volume                   = {24},

  Abstract                 = {This study aimed to investigate the difference in driving performance between drivers’ attention on the head-up display (HUD)/road under low/high road conditions via a driving simulator experiment. Experimental driving included four driving scenarios with attention-on-the-HUD followed by attention-on-the-road or vice versa under high or low driving load conditions. Each scenario took about a 30-min driving consisting of two 15-min sections for each attention location. Forty-eight participants, divided into four groups, drove one of the four scenarios once. Besides driving safely within speed limit, participants were also required to perform detection task and speed limit sign response task. Results revealed that drivers paying attention to the HUD, under both low and high driving load conditions, reacted faster to speed limit sign changes than when paying attention to the road. In addition, attention-to-the-HUD under low driving load condition caused the smallest variation in steering wheel angle and lateral acceleration. These differences can be attributed to the driver's enhanced awareness and the cognitive capture effect, and tended to diminish with increasing driving workload. Finally, attention shift of drivers and the so-called novelty effect for using new technology product were also found.},
  Doi                      = {10.1016/j.displa.2004.01.001},
  Keywords                 = {Attention shift;
 Cognitive capture;
 Driving simulator;
 Head-up display;
 Novelty effect},
  Owner                    = {Cassio},
  Publisher                = {Elsevier},
  Review                   = {drivers use 90% of their vision in obtaining relateed driving information
HDD -> may have negative effects
HUD may reduce frequency and duration of the driver's eyes-off-the-road -> feasible and superior alternative or auciliary visual display interface
HUD in road traffic much more complicated than in open sky
people tend to look less on hud than hdd -> eyes more on road
Shrink effect on close objects with HUD (distance is overestimated)
HUD:
 -> respond faster to unanticipated road events under low and high driving loads
-> low load improved driving behaviours
-> increate mental load
-> increased attention when switching from road to hud, and vice versa (other studies different)
-> no familarity -> not better},
  Timestamp                = {2014.05.15}
}

@Article{10.1109/MMUL.2011.14,
  Title                    = {Multimodal Input in the Car, Today and Tomorrow},
  Author                   = {Christian Müller and Garrett Weinberg},
  Journal                  = {IEEE Multimedia},
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {98-103},
  Volume                   = {18},

  Abstract                 = {With the increased functionality offered by in-vehicle systems, multimodal input is emerging as an effective means of interaction to minimize driver distraction. This article describes the current state of this technology for automotive applications, various ways to combine modalities, and outlooks toward the future.},
  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/MMUL.2011.14},
  ISSN                     = {1070-986X},
  Owner                    = {Cassio},
  Publisher                = {IEEE Computer Society},
  Review                   = {Tasks taking more than 15seconds while stationary are forbidden in motion
Speech was alway an optional (separated/independant) way to interact in addition to the manual
Fusion of mutliple modalities allows better accuracy,
Driver Activity Load Index for measurment
->Allow to user to use any modality he wants at any time without thinking about what he can do},
  Timestamp                = {2014.05.15}
}

@Article{Nawaz:2008:IDC:2275720.2279600,
  Title                    = {Infotainment Devices Control by Eye Gaze and Gesture Recognition Fusion},
  Author                   = {Nawaz, T. and Mian, M. and Habib, H.},
  Journal                  = {IEEE Trans. on Consum. Electron.},
  Year                     = {2008},

  Month                    = may,
  Number                   = {2},
  Pages                    = {277--282},
  Volume                   = {54},

  Abstract                 = {This paper presents a novel concept for controlling the consumer devices such as MP3 player or other daily life appliances by fusion of eye gaze and gesture recognition methodologies. Such system is deployable for virtually controlling the consumer devices anywhere in the home and office. The usability of the system is also tested with patients at the intensive care units of the hospitals where the patients may not able to operate the consumer devices in a
regular way. The proposed system consists of a video processing based embedded system, a CCD camera and situated display. Physical control options are displayed over the situated display. Selection / de-selection process of displayed control option over situated display is accomplished by analyzing the video sequence captured by the CCD camera. Eye gaze estimation and head gesture recognition algorithms analyze the video sequence and reveal the control command that has to be sent to the infotainment or other consumer devices.},
  Acmid                    = {2279600},
  Address                  = {Piscataway, NJ, USA},
  Doi                      = {10.1109/TCE.2008.4560086},
  ISSN                     = {0098-3063},
  Issue_date               = {May 2008},
  Numpages                 = {6},
  Owner                    = {Cassio},
  Publisher                = {IEEE Press},
  Timestamp                = {2014.05.15},
  Url                      = {http://dx.doi.org/10.1109/TCE.2008.4560086}
}

@InProceedings{Prendinger:2007:GIA:1255047.1255064,
  Title                    = {Gaze-based Infotainment Agents},
  Author                   = {Prendinger, Helmut and Eichner, Tobias and Andr{\'e}, Elisabeth and Ishizuka, Mitsuru},
  Booktitle                = {Proceedings of the International Conference on Advances in Computer Entertainment Technology},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Pages                    = {87--90},
  Publisher                = {ACM},
  Series                   = {ACE '07},

  Abstract                 = {We propose an infotainment presentation system that relies on eyegaze as an intuitive and unobtrusive input modality. The system analyzes eye movements in real-time to infer users' attention, visual interest, and preference regarding interface objects. The application consists of a virtual showroom where a team of two highly realistic 3D agents presents product items in an entertaining and attractive way. The presentation flow adapts to the user's attentiveness and interest, or lack thereof, and thus provides a more personalized and user-attentive experience of the presentation.},
  Acmid                    = {1255064},
  Doi                      = {10.1145/1255047.1255064},
  ISBN                     = {978-1-59593-640-0},
  Keywords                 = {eye tracking, interest recognition, multi-modal presentation, preference detection},
  Location                 = {Salzburg, Austria},
  Numpages                 = {4},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.15},
  Url                      = {http://doi.acm.org/10.1145/1255047.1255064}
}

@InProceedings{schmidt2010automotive,
  Title                    = {Automotive user interfaces: human computer interaction in the car},
  Author                   = {Schmidt, Albrecht and Dey, Anind K and Kun, Andrew L and Spiessl, Wolfgang},
  Booktitle                = {CHI'10 Extended Abstracts on Human Factors in Computing Systems},
  Year                     = {2010},
  Organization             = {ACM},
  Pages                    = {3177--3180},

  Abstract                 = {Cars have become complex interactive systems. Mechanical controls and electrical systems are transformed to the digital realm. It is common that drivers operate a vehicle and, at the same time, interact with a variety of devices and applications. Texting while driving, looking up an address for the navigation system, and taking a phone call are just some common examples that add value for the driver, but also increase the risk of driving. Novel interaction technologies create many opportunities for designing useful and attractive in-car user interfaces. With technologies that assist the user in driving, such as assistive cruise control and lane keeping, the user interface is essential to the way people perceive the driving experience. New means for user interface development and interaction design are required as the number of factors influencing the design space for automotive user interfaces is increasing. In comparison to other domains, a trial and error approach while the product is already in the market is not acceptable as the cost of failure may be fatal. User interface design in the automotive domain is relevant across many areas ranging from primary driving control, to assisted functions, to navigation, information services, entertainment and games.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.15}
}

@TechReport{steinfeld1995driver,
  Title                    = {Driver response times to full-windshield, head-up displays for navigation and vision enhancement},
  Author                   = {Steinfeld, Aaron and Green, Paul E},
  Institution              = {University of Michigan, Transportation Research Institute},
  Year                     = {1995},

  Owner                    = {Cassio},
  Timestamp                = {2014.05.15}
}

@InProceedings{velichkovsky1997towards,
  Title                    = {Towards gaze-mediated interaction: Collecting solutions of the “Midas touch problem”},
  Author                   = {Velichkovsky, Boris and Sprenger, Andreas and Unema, Pieter},
  Booktitle                = {Human-Computer Interaction INTERACT’97},
  Year                     = {1997},
  Organization             = {Springer},
  Pages                    = {509--516},

  Abstract                 = {For a development of truly user-centered interfaces we need to take into account not only generic characteristics of human beings but also actual dynamics of attention and intentions of persons involved in an interaction. Modern eyetracking methods are indispensable tools in such a development, as they allow the use of eye movement data for control of output devices, for gaze-contingent image processing and for desambiguation of verbal as well as nonverbal information. The main obstacle on the way to these applications is the so-called “Midas touch problem”: how to differentiate “attentive” saccades with intended goal of communication from the lower level eye movements that are just random or provoked by external stimulation? We report results of our investigations of the problem and present a solution based on a functional classification of fixations correlated with their duration. Several additional solutions are also considered together with the data on the trainability of the human oculomotor system.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.14}
}

