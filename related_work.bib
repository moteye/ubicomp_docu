% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{bretzner2005towards,
  Title                    = {Towards low-cost systems for measuring visual cues of driver fatigue and inattention in automotive applications},
  Author                   = {Bretzner, Lars and Krantz, Martin},
  Booktitle                = {Vehicular Electronics and Safety, 2005. IEEE International Conference on},
  Year                     = {2005},
  Organization             = {IEEE},
  Pages                    = {161--164},

  Abstract                 = {Recent studies show that driver fatigue and
inattention are major causes of fatal road accidents. A large
number of these accidents could be avoided if the vehicles
were equipped with a) sensors that reliably could monitor the
attention and alertness cues of the drivers, and b) systems that,
based on the sensor input, could warn the drivers in time.
Today, camera-based systems can measure such attention cues,
and are tested in automotive in-cabin installations with two or
more cameras. Camera-based solutions are non-intrusive, they
can measure head position and orientation as well as gaze
direction and eyelid closure, and they can be used to identify
the driver for other tasks. However until now, no camera-based
system has met the requirements from the car manufacturers on
compactness and cost. Here we present the principles behind
Smart Eye AntiSleep, which is a compact one-camera system
specially designed for automotive in-cabin applications.
AntiSleep measures 3D head position, head orientation, gaze
direction and eyelid closures. AntiSleep uses one camera and
two IR-flashes. With its own illumination, AntiSleep is highly
robust to all natural illumination conditions and can efficiently
handle disturbing reflexes in eyeglasses. The system is fully
automatic and requires no manual intervention during initial-
isation.},
  File                     = {:papers/10.1.1.137.3735.pdf:PDF},
  Owner                    = {tamino},
  Timestamp                = {2014.05.11}
}

@Article{horrey2004driving,
  Title                    = {Driving and side task performance: The effects of display clutter, separation, and modality},
  Author                   = {Horrey, William J and Wickens, Christopher D},
  Journal                  = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {611--624},
  Volume                   = {46},

  Abstract                 = {In-vehicle technologies (IVTs) create additional tasks for the driver. To the extent that these devices degrade driving performance, there will be safety concerns. This study examines the effects of display clutter from overlay, display separation, and modality on driving and IVT task performance. In a fixed-base simulator, 22 drivers drove different routes and responded to infrequent, unexpected road hazards while engaging in a phone number task presented by different displays. Visual displays were located on a head-up (overlaid on the visual horizon or adjacently, just above the vehicle hood) or head-down display (HDD) located near the midconsole. Alternatively, digits were presented auditorily. In general, there were no differences in performance for the adjacent and overlay displays; however, there were costs associated with the HDD and auditory display for some measures. In particular, responses to hazard events were slowed when drivers used the HDD. Overall, the adjacent display best supported performance on all relevant tasks. Potential applications of this research include the design of IVTs with respect to location and modality.},
  Doi                      = {10.1518/hfes.46.4.611.56805},
  Owner                    = {Cassio},
  Publisher                = {Sage Publications},
  Timestamp                = {2014.05.15}
}

@InProceedings{huckauf2005you,
  Title                    = {What you don't look at is what you get: anti-saccades can reduce the midas touch-problem},
  Author                   = {Huckauf, Anke and Goettel, Timo and Heinbockel, Malte and Urbina, Mario},
  Booktitle                = {Proceedings of the 2nd symposium on Applied perception in graphics and visualization},
  Year                     = {2005},
  Organization             = {ACM},
  Pages                    = {170--170},

  Abstract                 = {Controlling computers by eyes can provide a fast and efficient alternative to the computer mouse. However, in gaze controlled systems object selection is still suboptimal: Dwell times on a certain object typically used to elicit the selection of this object can cause unintended selections; a problem known as Midas Touch. For our suggested alternative approach using anti-saccades for selection, highlighted objects are copied to one side of the object. The object is selected when fixating to the side opposed to that copy requiring to inhibit an automatic response towards new objects. A user study revealed shorter task completion times, but more errors when selecting objects by anti-saccades relative to selection by individually and task-specifically adapted dwell times. With certain improvements, anti-saccades can become a serious alternative to dwell times.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.15}
}

@Misc{iti1999method,
  Title                    = {Method and apparatus for automatically configuring a system based on a user's monitored system interaction and preferred system access times},

  Author                   = {Iti, Jean M Goldschmidt and Hackson, David N and Moore, Kenneth Alan and Shah-Nazaroff, Anthony A and Watts, E Michael and Williams, Christopher D},
  Month                    = nov # {~2},
  Note                     = {US Patent 5,977,964},
  Year                     = {1999},

  Abstract                 = {In accordance with the teachings of the present invention, a method and apparatus for automatically configuring a system based on a user's monitored system interaction and preferred system access times is provided. According to one embodiment, a user profile corresponding to the user is updated based at least in part on the monitored user interaction with the system. Preferred system access times of the user are identified based at least in part on the user profile, and the system is automatically configured based at least in part on the user profile and the user's preferred system access times.},
  Owner                    = {tamino},
  Publisher                = {Google Patents},
  Timestamp                = {2014.05.15}
}

@InProceedings{jacob1990you,
  Title                    = {What you look at is what you get: eye movement-based interaction techniques},
  Author                   = {Jacob, Robert JK},
  Booktitle                = {Proceedings of the SIGCHI conference on Human factors in computing systems},
  Year                     = {1990},
  Organization             = {ACM},
  Pages                    = {11--18},

  Abstract                 = {In seeking hitherto-unused methods by which users and computers can communicate, we investigate the usefulness of eye movements as a fast and convenient auxiliary user-to-computer communication mode. The barrier to exploiting this medium has not been eye-tracking technology but the study of interaction techniques that incorporate eye movements into the user-computer dialogue in a natural and unobtrusive way. This paper discusses some of the human factors and technical considerations that arise in trying to use eye movements as an input medium, describes our approach and the first eye movement-based interaction techniques that we have devised and implemented in our laboratory, and reports our experiences and observations on them.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.14}
}

@Article{ji2002real,
  Title                    = {Real-time eye, gaze, and face pose tracking for monitoring driver vigilance},
  Author                   = {Ji, Qiang and Yang, Xiaojie},
  Journal                  = {Real-Time Imaging},
  Year                     = {2002},
  Number                   = {5},
  Pages                    = {357--377},
  Volume                   = {8},

  Abstract                 = {This paper describes a real-time prototype computer vision system for monitoring driver
vigilance. The main components of the system consists of a remotely located video CCD
camera, a specially designed hardware system for real-time image acquisition and for
controlling the illuminator and the alarm system, and various computer vision algorithms for
simultaneously, real-time and non-intrusively monitoring various visual bio-behaviors that
typically characterize a driver’s level of vigilance. The visual behaviors include eyelid movement,
face orientation, and gaze movement (pupil movement). The system was tested in a simulating
environment with subjects of different ethnic backgrounds, different genders, ages, with/without
glasses, and under different illumination conditions, and it was found very robust, reliable and
accurate.},
  File                     = {:papers/10.1.1.408.8589.pdf:PDF},
  Owner                    = {tamino},
  Publisher                = {Elsevier},
  Timestamp                = {2014.05.11}
}

@InProceedings{kern2009design,
  Title                    = {Design space for driver-based automotive user interfaces},
  Author                   = {Kern, Dagmar and Schmidt, Albrecht},
  Booktitle                = {Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
  Year                     = {2009},
  Organization             = {ACM},
  Pages                    = {3--10},

  Abstract                 = {Over the last 100 years it has become much easier to operate a car. However in recent years the number of functions a user can control while driving has greatly increased. Infotainment, entertainment and comfort systems as well as driver assistance contribute to this trend. Interaction with these systems plays an important role, as on one hand this can improve the user experience while driving but on the other hand it may distract from the primary task of driving. User interfaces in cars differ regarding the number of input and output devices and their placement in the car to a great extent. In this paper, we introduce a first design space for driver-based automotive user interfaces that allows a comprehensive description of input and output devices in a car with regard to placement and modality. This design space is intended to provide a basis for analyzing and discussing different user interface arrangements in cars, to compare alternative user interface setups, and to identify new opportunities for interaction and placement of controls. We present a graphical representation of the design space and discuss its usage in detail based on several examples. To assess the completeness of the proposed design space we used it to classify and compare user interfaces from more than 100 cars shown at IAA2007, cars from the BMW museum, and from the A2Mac1 image database.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.15}
}

@InProceedings{liu2002real,
  Title                    = {Real-time eye detection and tracking for driver observation under various light conditions},
  Author                   = {Liu, Xia and Xu, Fengliang and Fujimura, Kikuo},
  Booktitle                = {Intelligent Vehicle Symposium, 2002. IEEE},
  Year                     = {2002},
  Organization             = {IEEE},
  Pages                    = {344--351},
  Volume                   = {2},

  Abstract                 = {Eye tracking is one of the key technologies for future driver assistance systems since human eyes contain much information about the driver's condition such as gaze, attention level, and fatigue level. Thus, nonintrusive methods for eye detection and tracking are important for many applications of vision-based driver-automotive interaction. One problem common to many eye tracking methods proposed so far is their sensitivity to lighting condition change. This tends to significantly limit their scope for automotive applications. In this paper we present a new realtime eye detection and tracking method that works under variable and realistic lighting conditions. By combining imaging by using IR light and appearance-based object recognition techniques, our method can robustly track eyes even when the pupils are not very bright due to significant external illumination interferences. The appearance model is incorporated in both eye detection and tracking via the use of a support vector machine and mean shift tracking. Our experimental results show the feasibility of our approach and the validity for the method is extended for drivers wearing sunglasses.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.11}
}

@Article{liu2003effects,
  Title                    = {Effects of using head-up display in automobile context on attention demand and driving performance},
  Author                   = {Liu, Yung-Ching},
  Journal                  = {Displays},
  Year                     = {2003},
  Number                   = {4},
  Pages                    = {157--165},
  Volume                   = {24},

  Abstract                 = {This study aimed to investigate the difference in driving performance between drivers’ attention on the head-up display (HUD)/road under low/high road conditions via a driving simulator experiment. Experimental driving included four driving scenarios with attention-on-the-HUD followed by attention-on-the-road or vice versa under high or low driving load conditions. Each scenario took about a 30-min driving consisting of two 15-min sections for each attention location. Forty-eight participants, divided into four groups, drove one of the four scenarios once. Besides driving safely within speed limit, participants were also required to perform detection task and speed limit sign response task. Results revealed that drivers paying attention to the HUD, under both low and high driving load conditions, reacted faster to speed limit sign changes than when paying attention to the road. In addition, attention-to-the-HUD under low driving load condition caused the smallest variation in steering wheel angle and lateral acceleration. These differences can be attributed to the driver's enhanced awareness and the cognitive capture effect, and tended to diminish with increasing driving workload. Finally, attention shift of drivers and the so-called novelty effect for using new technology product were also found.},
  Doi                      = {10.1016/j.displa.2004.01.001},
  Keywords                 = {Attention shift;
 Cognitive capture;
 Driving simulator;
 Head-up display;
 Novelty effect},
  Owner                    = {Cassio},
  Publisher                = {Elsevier},
  Timestamp                = {2014.05.15}
}

@Article{Nawaz:2008:IDC:2275720.2279600,
  Title                    = {Infotainment Devices Control by Eye Gaze and Gesture Recognition Fusion},
  Author                   = {Nawaz, T. and Mian, M. and Habib, H.},
  Journal                  = {IEEE Trans. on Consum. Electron.},
  Year                     = {2008},

  Month                    = may,
  Number                   = {2},
  Pages                    = {277--282},
  Volume                   = {54},

  Acmid                    = {2279600},
  Address                  = {Piscataway, NJ, USA},
  Doi                      = {10.1109/TCE.2008.4560086},
  ISSN                     = {0098-3063},
  Issue_date               = {May 2008},
  Numpages                 = {6},
  Owner                    = {Cassio},
  Publisher                = {IEEE Press},
  Timestamp                = {2014.05.15},
  Url                      = {http://dx.doi.org/10.1109/TCE.2008.4560086}
}

@InProceedings{Prendinger:2007:GIA:1255047.1255064,
  Title                    = {Gaze-based Infotainment Agents},
  Author                   = {Prendinger, Helmut and Eichner, Tobias and Andr{\'e}, Elisabeth and Ishizuka, Mitsuru},
  Booktitle                = {Proceedings of the International Conference on Advances in Computer Entertainment Technology},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Pages                    = {87--90},
  Publisher                = {ACM},
  Series                   = {ACE '07},

  Abstract                 = {We propose an infotainment presentation system that relies on eyegaze as an intuitive and unobtrusive input modality. The system analyzes eye movements in real-time to infer users' attention, visual interest, and preference regarding interface objects. The application consists of a virtual showroom where a team of two highly realistic 3D agents presents product items in an entertaining and attractive way. The presentation flow adapts to the user's attentiveness and interest, or lack thereof, and thus provides a more personalized and user-attentive experience of the presentation.},
  Acmid                    = {1255064},
  Doi                      = {10.1145/1255047.1255064},
  ISBN                     = {978-1-59593-640-0},
  Keywords                 = {eye tracking, interest recognition, multi-modal presentation, preference detection},
  Location                 = {Salzburg, Austria},
  Numpages                 = {4},
  Owner                    = {Cassio},
  Timestamp                = {2014.05.15},
  Url                      = {http://doi.acm.org/10.1145/1255047.1255064}
}

@InProceedings{schmidt2010automotive,
  Title                    = {Automotive user interfaces: human computer interaction in the car},
  Author                   = {Schmidt, Albrecht and Dey, Anind K and Kun, Andrew L and Spiessl, Wolfgang},
  Booktitle                = {CHI'10 Extended Abstracts on Human Factors in Computing Systems},
  Year                     = {2010},
  Organization             = {ACM},
  Pages                    = {3177--3180},

  Abstract                 = {Cars have become complex interactive systems. Mechanical controls and electrical systems are transformed to the digital realm. It is common that drivers operate a vehicle and, at the same time, interact with a variety of devices and applications. Texting while driving, looking up an address for the navigation system, and taking a phone call are just some common examples that add value for the driver, but also increase the risk of driving. Novel interaction technologies create many opportunities for designing useful and attractive in-car user interfaces. With technologies that assist the user in driving, such as assistive cruise control and lane keeping, the user interface is essential to the way people perceive the driving experience. New means for user interface development and interaction design are required as the number of factors influencing the design space for automotive user interfaces is increasing. In comparison to other domains, a trial and error approach while the product is already in the market is not acceptable as the cost of failure may be fatal. User interface design in the automotive domain is relevant across many areas ranging from primary driving control, to assisted functions, to navigation, information services, entertainment and games.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.15}
}

@TechReport{steinfeld1995driver,
  Title                    = {Driver response times to full-windshield, head-up displays for navigation and vision enhancement},
  Author                   = {Steinfeld, Aaron and Green, Paul E},
  Institution              = {University of Michigan, Transportation Research Institute},
  Year                     = {1995},

  Owner                    = {Cassio},
  Timestamp                = {2014.05.15}
}

@InProceedings{velichkovsky1997towards,
  Title                    = {Towards gaze-mediated interaction: Collecting solutions of the “Midas touch problem”},
  Author                   = {Velichkovsky, Boris and Sprenger, Andreas and Unema, Pieter},
  Booktitle                = {Human-Computer Interaction INTERACT’97},
  Year                     = {1997},
  Organization             = {Springer},
  Pages                    = {509--516},

  Abstract                 = {For a development of truly user-centered interfaces we need to take into account not only generic characteristics of human beings but also actual dynamics of attention and intentions of persons involved in an interaction. Modern eyetracking methods are indispensable tools in such a development, as they allow the use of eye movement data for control of output devices, for gaze-contingent image processing and for desambiguation of verbal as well as nonverbal information. The main obstacle on the way to these applications is the so-called “Midas touch problem”: how to differentiate “attentive” saccades with intended goal of communication from the lower level eye movements that are just random or provoked by external stimulation? We report results of our investigations of the problem and present a solution based on a functional classification of fixations correlated with their duration. Several additional solutions are also considered together with the data on the trainability of the human oculomotor system.},
  Owner                    = {tamino},
  Timestamp                = {2014.05.14}
}

